{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102966,"databundleVersionId":12412856,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom PIL import Image\n\n# Define dataset and label paths\ntraining_images_directory = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\ntest_images_directory = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\ntraining_labels_csv_path = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv'\ntest_ids_csv_path = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv'\n\n# Load training labels\nlabels_dataframe = pd.read_csv(training_labels_csv_path)\n\n# Stratified split for training and validation\ntraining_dataframe, validation_dataframe = train_test_split(\n    labels_dataframe,\n    test_size=0.2,\n    stratify=labels_dataframe['label'],\n    random_state=42\n)\n\n# Image transformations\nimage_transformation_pipeline = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet statistics\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Custom dataset for training and validation\nclass SoilImageDataset(Dataset):\n    def __init__(self, dataframe, image_directory, transform=None):\n        self.dataframe = dataframe\n        self.image_directory = image_directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        image_filename = self.dataframe.iloc[index]['image_id']\n        image_label = int(self.dataframe.iloc[index]['label'])\n        full_image_path = os.path.join(self.image_directory, image_filename)\n        image = Image.open(full_image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, image_label\n\n# Create datasets and dataloaders\ntraining_dataset = SoilImageDataset(training_dataframe, training_images_directory, image_transformation_pipeline)\nvalidation_dataset = SoilImageDataset(validation_dataframe, training_images_directory, image_transformation_pipeline)\n\ntraining_dataloader = DataLoader(training_dataset, batch_size=32, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n\n# Select computation device\ncomputation_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load pretrained ResNet18 model and modify final layer\nbinary_classification_model = models.resnet18(pretrained=True)\nbinary_classification_model.fc = nn.Sequential(\n    nn.Linear(binary_classification_model.fc.in_features, 1),\n    nn.Sigmoid()\n)\nbinary_classification_model = binary_classification_model.to(computation_device)\n\n# Define binary classification loss and optimizer\nbinary_loss_function = nn.BCELoss()\nadam_optimizer = optim.Adam(binary_classification_model.parameters(), lr=1e-4)\n\n# Training function with validation\ndef train_soil_model(model, train_loader, val_loader, total_epochs=10):\n    for epoch in range(total_epochs):\n        model.train()\n        cumulative_training_loss = 0.0\n\n        for batch_images, batch_labels in train_loader:\n            batch_images = batch_images.to(computation_device)\n            batch_labels = batch_labels.float().unsqueeze(1).to(computation_device)\n\n            adam_optimizer.zero_grad()\n            output_probabilities = model(batch_images)\n            batch_loss = binary_loss_function(output_probabilities, batch_labels)\n            batch_loss.backward()\n            adam_optimizer.step()\n\n            cumulative_training_loss += batch_loss.item()\n\n        # Validation loop\n        model.eval()\n        validation_predictions = []\n        validation_targets = []\n\n        with torch.no_grad():\n            for batch_images, batch_labels in val_loader:\n                batch_images = batch_images.to(computation_device)\n                batch_labels = batch_labels.to(computation_device)\n                output_probabilities = model(batch_images)\n                predicted_labels = (output_probabilities > 0.5).int().cpu().numpy()\n                validation_predictions.extend(predicted_labels.flatten())\n                validation_targets.extend(batch_labels.cpu().numpy())\n\n        validation_f1 = f1_score(validation_targets, validation_predictions)\n        print(f\"Epoch {epoch+1}/{total_epochs}, Training Loss: {cumulative_training_loss:.4f}, Validation F1 Score: {validation_f1:.4f}\")\n\n# Train the model\ntrain_soil_model(binary_classification_model, training_dataloader, validation_dataloader, total_epochs=10)\n\n# Load test image IDs\ntest_image_ids_dataframe = pd.read_csv(test_ids_csv_path)\n\n# Custom dataset for test images\nclass SoilTestImageDataset(Dataset):\n    def __init__(self, dataframe, image_directory, transform=None):\n        self.dataframe = dataframe\n        self.image_directory = image_directory\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        image_filename = self.dataframe.iloc[index]['image_id']\n        full_image_path = os.path.join(self.image_directory, image_filename)\n        image = Image.open(full_image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, image_filename\n\n# Create test dataset and dataloader\ntest_dataset = SoilTestImageDataset(test_image_ids_dataframe, test_images_directory, image_transformation_pipeline)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Generate predictions\nbinary_classification_model.eval()\ntest_predictions = []\n\nwith torch.no_grad():\n    for batch_images, image_names in test_dataloader:\n        batch_images = batch_images.to(computation_device)\n        output_probabilities = binary_classification_model(batch_images)\n        predicted_labels = (output_probabilities > 0.5).int().cpu().numpy().flatten()\n        for image_name, predicted_label in zip(image_names, predicted_labels):\n            test_predictions.append({'image_id': image_name, 'label': predicted_label})\n\n# Create and save submission file\nsubmission_dataframe = pd.DataFrame(test_predictions)\nsubmission_dataframe.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as 'submission.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T18:02:32.003703Z","iopub.execute_input":"2025-05-24T18:02:32.003933Z","iopub.status.idle":"2025-05-24T18:29:59.399781Z","shell.execute_reply.started":"2025-05-24T18:02:32.003914Z","shell.execute_reply":"2025-05-24T18:29:59.397463Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 150MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Training Loss: 8.0836, Validation F1 Score: 0.9597\nEpoch 2/10, Training Loss: 1.3351, Validation F1 Score: 1.0000\nEpoch 3/10, Training Loss: 0.7120, Validation F1 Score: 1.0000\nEpoch 4/10, Training Loss: 0.4389, Validation F1 Score: 1.0000\nEpoch 5/10, Training Loss: 0.3365, Validation F1 Score: 1.0000\nEpoch 6/10, Training Loss: 0.2537, Validation F1 Score: 1.0000\nEpoch 7/10, Training Loss: 0.2382, Validation F1 Score: 1.0000\nEpoch 8/10, Training Loss: 0.1856, Validation F1 Score: 1.0000\nEpoch 9/10, Training Loss: 0.1281, Validation F1 Score: 1.0000\nEpoch 10/10, Training Loss: 0.1146, Validation F1 Score: 1.0000\nSubmission file saved as 'submission.csv'\n","output_type":"stream"}],"execution_count":1}]}